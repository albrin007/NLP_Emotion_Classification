import pandas as pd
import nltk
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

# Download necessary resources from nltk
nltk.download('punkt')
nltk.download('stopwords')

# Load the dataset
try:
    df = pd.read_csv('/content/nlp_dataset.csv')  # Replace with the correct path
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: Dataset file not found.")
    raise

# Verify and clean column names
df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces
print("Dataset Columns:", df.columns)

# Replace 'Comment' and 'Emotion' with actual column names
text_column = 'Comment'  # Update to the correct column name for text
emotion_column = 'Emotion'  # Update to the correct column name for emotion/label

# Validate column names
if text_column not in df.columns or emotion_column not in df.columns:
    raise ValueError(f"Dataset must contain '{text_column}' and '{emotion_column}' columns.")

# Handle missing values
df = df.dropna(subset=[text_column, emotion_column])  # Drop rows with missing data
df[text_column] = df[text_column].fillna("")  # Fill missing text with an empty string

# Preview dataset
print(df[[text_column, emotion_column]].head())

# Define preprocessing function
def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()  # Convert to lowercase
    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation
    tokens = word_tokenize(text)  # Tokenize
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    return ' '.join(tokens)

# Apply preprocessing
df['cleaned_text'] = df[text_column].apply(preprocess_text)
print(df[[text_column, 'cleaned_text']].head())

# Initialize TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_text']).toarray()
y = df[emotion_column]  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
nb_accuracy = nb_model.score(X_test, y_test)
print(f'Naive Bayes Accuracy: {nb_accuracy:.2f}')

# Train SVM model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)
svm_accuracy = svm_model.score(X_test, y_test)
print(f'SVM Accuracy: {svm_accuracy:.2f}')

# Evaluate models
y_pred_nb = nb_model.predict(X_test)
y_pred_svm = svm_model.predict(X_test)

accuracy_nb = accuracy_score(y_test, y_pred_nb)
accuracy_svm = accuracy_score(y_test, y_pred_svm)

f1_nb = f1_score(y_test, y_pred_nb, average='weighted')
f1_svm = f1_score(y_test, y_pred_svm, average='weighted')

print(f'Naive Bayes Accuracy: {accuracy_nb:.2f}, F1-Score: {f1_nb:.2f}')
print(f'SVM Accuracy: {accuracy_svm:.2f}, F1-Score: {f1_svm:.2f}')
